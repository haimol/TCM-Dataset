# TCM Question Database

A comprehensive Traditional Chinese Medicine (TCM) question dataset containing both extracted and AI-generated multiple-choice questions for medical education and research.

## 📊 Dataset Overview

**Total Questions: 13,750**
- **Extracted Data:** 11,349 questions (original sources)
- **Generated Data:** 2,401 questions (AI-generated using TCM-Align)

## 📁 Repository Structure

```
TCM-DATA/
├── Extracted-Data/          # Original question collections
│   ├── 中医内科学/           # TCM Internal Medicine (897 questions)
│   ├── 中医外科学/           # TCM Surgery (899 questions)  
│   ├── 中医妇科学/           # TCM Gynecology (899 questions)
│   ├── 中医儿科学/           # TCM Pediatrics (500 questions)
│   ├── questions.json      # Consolidated questions (325 questions)
│   ├── 中医住培题.json      # Resident physician training questions (7,829 questions)
│   └── *-metadata.json     # Metadata files documenting structure
└── Generated-Data/          # AI-generated using TCM-Align methodology
    ├── 实用中医内科学.json    # Practical TCM Internal Medicine (694 questions)
    ├── 中医基础理论.json      # TCM Basic Theory (374 questions)
    ├── 中医妇科学.json        # TCM Gynecology (378 questions)
    ├── 中医诊断学.json        # TCM Diagnostics (320 questions)
    ├── 中医儿科学.json        # TCM Pediatrics (281 questions)
    ├── 中医眼科学.json        # TCM Ophthalmology (210 questions)
    └── 中医外科学.json        # TCM Surgery (144 questions)
```

## 🗂️ Data Categories

### Extracted Data (11,349 questions)

Original questions collected from various TCM educational sources:

| Category | A1-Type Questions | A2-Type Questions | Total |
|----------|------------------|------------------|-------|
| 中医内科学 (TCM Internal Medicine) | 600 | 297 | 897 |
| 中医外科学 (TCM Surgery) | 599 | 300 | 899 |
| 中医妇科学 (TCM Gynecology) | 599 | 300 | 899 |
| 中医儿科学 (TCM Pediatrics) | 300 | 200 | 500 |
| **Additional Sources** | | | **8,154** |
| 住培题 (Resident Physician Training) | - | - | 7,829 |
| General Questions | - | - | 325 |

**Question Types:**
- **A1-Type**: Single best answer multiple choice (direct knowledge recall)
- **A2-Type**: Case-based clinical scenario questions (application of knowledge)

### Generated Data (2,401 questions)

AI-generated questions using the **TCM-Align** methodology across 7 core TCM domains:

| Domain | Questions | Description |
|--------|-----------|-------------|
| 实用中医内科学 | 694 | Practical TCM Internal Medicine |
| 中医妇科学 | 378 | TCM Gynecology |
| 中医基础理论 | 374 | TCM Basic Theory |
| 中医诊断学 | 320 | TCM Diagnostics |
| 中医儿科学 | 281 | TCM Pediatrics |
| 中医眼科学 | 210 | TCM Ophthalmology |
| 中医外科学 | 144 | TCM Surgery |

## 🤖 TCM-Align Methodology

The Generated-Data was created using **TCM-Align**, a novel framework for curriculum-aligned MCQ generation published at UbiComp Companion '25.

### Key Features

1. **Heuristic-based Knowledge Extraction**
   - Parses TCM textbooks along chapter/section boundaries
   - Maintains semantic coherence within question topics

2. **Dual-Summary Semantic Filtering**
   - Generates two independent summaries per text segment
   - Applies cosine similarity threshold (≥0.92) for quality control
   - Filters out hallucinated or low-confidence content

3. **MCQ Generator with Format Verification**
   - Creates structured multiple-choice questions with explanations
   - Ensures one correct answer and three plausible distractors
   - Maintains factual consistency with source material

### Quality Metrics

The TCM-Align framework achieves:
- **Source-Consistency Accuracy (SCA)**: 0.98 (98% factual accuracy)
- **94.3% improvement** in Explanation Richness vs. baseline methods
- **>10% gains** in most quality metrics including reasoning depth and distractor quality

### Source Materials

Generated from **Chinese National TCM Higher-Education Planned Textbooks series**, including:
- TCM Basic Theory
- TCM Diagnostics  
- TCM Internal Medicine
- TCM Surgery
- TCM Gynecology
- TCM Pediatrics
- TCM Ophthalmology

## 📋 Data Format

### Extracted Data Format
```json
{
    "query": "患者王某，男性，69岁。大便艰涩...",
    "choices": [
        "气秘",
        "冷秘", 
        "热秘",
        "虚秘",
        "实秘"
    ],
    "answers": ["虚秘"],
    "explanation": ""
}
```

### Generated Data Format (Enhanced)
```json
{
    "item_id": "实用中医内科学完整-0",
    "generated_question": {
        "question": "《内经》对中医内科学的贡献主要包括以下哪一项？",
        "options": [
            {
                "label": "A",
                "text": "创立了脏腑、经络、气血体系，提出病因病机学说",
                "correct": true
            },
            {
                "label": "B", 
                "text": "首次记载狂犬病的传播与预防方法",
                "correct": false
            }
        ],
        "explanation": "《内经》奠定了中医内科学的理论基础..."
    }
}
```

## 🎯 Use Cases

### Educational Applications
- **Medical School Training**: Comprehensive question banks for TCM curricula
- **Licensing Exam Preparation**: Practice questions for TCM certification
- **Continuing Education**: Professional development for TCM practitioners

### Research Applications  
- **NLP Model Training**: Large-scale domain-specific dataset for Chinese medical NLP
- **Question Generation Research**: Benchmark for automated MCQ generation
- **Educational Assessment**: Evaluation of TCM knowledge and reasoning

### Clinical Applications
- **Knowledge Assessment**: Evaluate TCM practitioners' competency
- **Diagnostic Training**: Case-based reasoning for clinical scenarios

## 📈 Dataset Statistics

| Metric | Extracted Data | Generated Data | Total |
|--------|---------------|---------------|-------|
| **Total Questions** | 11,349 | 2,401 | **13,750** |
| **Average Explanation Length** | Minimal | Rich & Detailed | - |
| **Question Types** | A1/A2 Mixed | MCQ with Explanations | - |
| **Domains Covered** | 4 Main + Others | 7 Core Domains | 8+ |
| **Quality Assurance** | Manual Collection | AI-Filtered (SCA=0.98) | - |

## 🔧 Getting Started

### Loading the Data

**Python Example:**
```python
import json

# Load extracted data
with open('Extracted-Data/1、中医内科学600道A1型题.json', 'r', encoding='utf-8') as f:
    extracted_questions = json.load(f)

# Load generated data  
with open('Generated-Data/实用中医内科学.json', 'r', encoding='utf-8') as f:
    generated_questions = json.load(f)

print(f"Extracted: {len(extracted_questions)} questions")
print(f"Generated: {len(generated_questions)} questions")
```

### Question Count Script
```bash
python count_questions.py
```
Generates `question_count_summary.txt` with detailed statistics.

## 📚 Citation

If you use this dataset in your research, please cite:

```bibtex
@inproceedings{lu2025tcmalign,
  title={TCM-Align: Curriculum-Aligned MCQ Generation for Traditional Chinese Medicine},
  author={Lu, Haimo and Liu, Li and Yuan, Jinliang and Zheng, Yawen and Wang, Zhenyu and Liu, Kebin},
  booktitle={Companion of the 2025 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages={1--6},
  year={2025},
  organization={ACM}
}
```

## 📄 License

This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).

## 🤝 Contributing

We welcome contributions to expand and improve this dataset:

1. **Additional Question Sources**: Submit new TCM questions with proper attribution
2. **Quality Improvements**: Report issues or suggest enhancements
3. **Translation**: Help translate questions to other languages
4. **Evaluation**: Provide feedback on question quality and difficulty

## 📞 Contact

For questions, suggestions, or collaboration opportunities:

- **Primary Contact**: Li Liu (liuli95@mail.tsinghua.edu.cn)
- **Lead Author**: Haimo Lu (haimolu@live.com)
- **Institution**: Tsinghua University, Beijing, China

## 🔗 Related Work

### TCM-Specific Datasets
- **TCM-QA** [9]: 574 questions extracted from BaiduWenKu for ChatGPT evaluation
- **TCMBench** [7]: 5,473 questions from TCM licensing exams for LLM benchmarking  
- **TCMD** [8]: 3,451 TCM QA pairs for large language model evaluation
- **Huatuo-26M** [12]: Large-scale Chinese medical QA dataset with 26M pairs
- **CMExam** [13]: 60,000+ questions from Chinese National Medical Licensing Examination

### LLM-Based Question Generation
- **SciQAG** [10]: Framework for auto-generated science QA datasets with fine-grained evaluation
- **MCQGen** [14]: LLM-driven MCQ generator using retrieval-augmented generation
- **EHR-DS-QA** [11]: Synthetic QA dataset from medical discharge summaries

### TCM-Specific Language Models
- **Qibo** [5]: Large language model specialized for Traditional Chinese Medicine
- **Zhongjing** [6]: Enhanced Chinese medical LLM with expert feedback and dialogue

### Foundational Technologies
- **GPT-4** [3]: Advanced language model used in our TCM-Align pipeline
- **Sentence-BERT** [21]: Sentence embeddings for semantic similarity computation
- **Self-Consistency** [22]: Chain of thought reasoning methodology applied in our dual-summary filtering

## ⚠️ Important Notes

1. **Educational Purpose**: This dataset is intended for educational and research use only
2. **Medical Disclaimer**: Not for clinical decision-making or patient care
3. **Quality Assurance**: Generated questions achieve 98% factual consistency but should be reviewed by domain experts for clinical applications
4. **Language**: Primarily in Chinese (Simplified), reflecting the source materials and target audience

## 📖 References

[3] Josh Achiam, Steven Adler, Sandhini Agarwal, et al. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.

[5] Heyi Zhang, Xin Wang, Zhaopeng Meng, Zhe Chen, Pengwei Zhuang, Yongzhe Jia, Dawei Xu, and Wenbin Guo. Qibo: A large language model for traditional chinese medicine. 2024.

[6] Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying Zan. Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. 2023.

[7] Wenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan, Huanran Zheng, Pengfei Wang, Changzhi Sun, and Xin Ma. TCMBench: A comprehensive benchmark for evaluating large language models in traditional chinese medicine. 2024.

[8] Ping Yu, Kaitao Song, Fengchen He, Ming Chen, and Jianfeng Lu. TCMD: A traditional chinese medicine qa dataset for evaluating large language models. *arXiv preprint arXiv:2406.04941*, 2024.

[9] Li Yizhen, Huang Shaohan, Qi Jiaxing, Quan Lei, Han Dongran, and Luan Zhongzhi. Exploring the comprehension of ChatGPT in traditional chinese medicine knowledge. 2024.

[10] Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster. SciQAG: A framework for auto-generated science question answering dataset with fine-grained evaluation. 2024.

[11] Konstantin Kotschenreuther. EHR-DS-QA: A synthetic QA dataset derived from medical discharge summaries for enhanced medical information retrieval systems. 2024.

[12] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. Huatuo-26M, a large-scale chinese medical QA dataset. 2023.

[13] Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, and Michael Lingzhi Li. Benchmarking large language models on CMExam – a comprehensive chinese medical exam dataset. 2023.

[14] Ching Nam Hang, Chee Wei Tan, and Pei-Duo Yu. MCQGen: A large language model-driven MCQ generator for personalized learning. *IEEE Access*, 12:102261–102273, 2024.

[21] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERT-networks. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing*. Association for Computational Linguistics, November 2019.

[22] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Nathan Scales, et al. Self-consistency improves chain of thought reasoning in language models. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2023.

---

**Last Updated**: December 2024  
**Dataset Version**: 1.0  
**Total Size**: ~15MB (JSON format) 